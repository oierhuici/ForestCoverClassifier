{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer,confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#Importamos los datos de entrenamiento\n",
    "data_train = pd.read_csv(\"../input/data_train.csv\").values\n",
    "#data_train = np.hstack((data_train[:,0:4],data_train[:,5:55])) #quitar feature 4\n",
    "\n",
    "#Importamos los datos de test\n",
    "data_test = pd.read_csv(\"../input/data_test.csv\").values\n",
    "\n",
    "#Separamos las features y las clases en subsets de train y test (20%) para hacer pruebas\n",
    "#x_train, x_test, y_train, y_test = train_test_split(data_train[:,0:54], data_train[:,54], test_size = 0.2, random_state = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Modelo Isolation Forest\n",
    "model = IsolationForest(contamination=\"auto\", random_state=30)\n",
    "\n",
    "# Entrena el modelo\n",
    "model.fit(data_train) #probar teniendo en cuenta solo variables no categoricas\n",
    "\n",
    "# Predice las etiquetas de outliers (1 para inliers, -1 para outliers) => Pasar a True inliers False outliers\n",
    "outlier_labels = model.predict(data_train)\n",
    "outlier_labels = np.asarray(np.where(outlier_labels==-1, 0, outlier_labels)).T\n",
    "outlier_labels = [bool(outlier_labels[i]) for i in range(len(outlier_labels))]\n",
    "\n",
    "#Filtramos el dataset\n",
    "data_train_inliers = data_train[outlier_labels]\n",
    "data_train = data_train_inliers\n",
    "\n",
    "#Separamos las features y las clases en subsets de train y test (20%) para hacer pruebas\n",
    "#x_train, x_test, y_train, y_test = train_test_split(data_train_inliers[:,0:54], data_train_inliers[:,54], test_size = 0.2, random_state = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost matrix and score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [[ 0,   5,  1, 1, 1,  1, 1],\n",
    "[10,   0,  1, 1, 1,  1, 1],\n",
    "[20,  20,  0, 5, 5, 50, 5],\n",
    "[20,  20, 10, 0, 1, 50, 5],\n",
    "[20, 100,  5, 1, 0,  5, 5],\n",
    "[ 5,  10, 10, 5, 1,  0, 1],\n",
    "[10,   5,  1, 1, 1,  1, 0]]\n",
    "\n",
    "def score_function (y, y_pred):\n",
    "     C = [[ 0,   5,  1, 1, 1,  1, 1],\n",
    "     [10,   0,  1, 1, 1,  1, 1],\n",
    "     [20,  20,  0, 5, 5, 50, 5],\n",
    "     [20,  20, 10, 0, 1, 50, 5],\n",
    "     [20, 100,  5, 1, 0,  5, 5],\n",
    "     [ 5,  10, 10, 5, 1,  0, 1],\n",
    "     [10,   5,  1, 1, 1,  1, 0]]\n",
    "     score = np.mean([C[int(i)][int(j)] for i, j in zip(y, y_pred)])\n",
    "     return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSeacrhCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# Configuración del clasificador XGBoost\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=7, random_state = 30, n_jobs = -1)\n",
    "\n",
    "# Definición de los parámetros a buscar en la cuadrícula\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [6, 30, 50],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_split_loss': [0,0.1,0.2]\n",
    "}\n",
    "\n",
    "\n",
    "scorer = make_scorer(score_func = score_function, greater_is_better = False)\n",
    "\n",
    "# Configuración del objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scorer, cv=3, verbose=3)\n",
    "\n",
    "# Definición de la matriz de costes\n",
    "def get_sample_weights(labels, cost_matrix):\n",
    "    # Calcula los pesos de muestra basados en la matriz de costes y las etiquetas\n",
    "    weights = np.zeros_like(labels, dtype=float)\n",
    "    for i, label in enumerate(labels):\n",
    "        weights[i] = sum(cost_matrix[int(label)])\n",
    "    return weights\n",
    "\n",
    "sample_weights = get_sample_weights(y_train, C)\n",
    "\n",
    "grid_search.fit(x_train, y_train, sample_weight = sample_weights)\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "print(\"Mejores parámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Mejor modelo entrenado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predicciones en el conjunto de prueba con el mejor modelo\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Score: {score_function(y_test,y_pred)}\\n')\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del clasificador XGBoost\n",
    "clf = xgb.XGBClassifier(\n",
    "                        random_state = 30,\n",
    "                        n_estimators = 200,\n",
    "                        learning_rate = 0.1,\n",
    "                        max_depth = 30,\n",
    "                        objective = 'multi:softmax',\n",
    "                        num_class = 7,\n",
    "                        verbosity = 2,\n",
    "                        subsample = 0.8,\n",
    "                        colsample_bytree= 1.0,\n",
    "                        min_split_loss = 0\n",
    "                        )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "y_proba = clf.predict_proba(x_test)\n",
    "weighted_costs = np.matmul(y_proba, np.array(C))\n",
    "y_pred = np.argmin(weighted_costs, axis = 1)\n",
    "\n",
    "# Evaluación del modelo\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Score: {score_function(y_test,y_pred)}\\n')\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del clasificador XGBoost\n",
    "clf = xgb.XGBClassifier(\n",
    "                        random_state = 30,\n",
    "                        n_estimators =  200,\n",
    "                        learning_rate = 0.1,\n",
    "                        max_depth = 30,\n",
    "                        objective = 'multi:softmax',\n",
    "                        num_class = 7,\n",
    "                        verbosity = 2,\n",
    "                        subsample = 0.8,\n",
    "                        colsample_bytree= 1.0,\n",
    "                        min_split_loss = 0\n",
    "                        )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "clf.fit(data_train[:,0:54], data_train[:,54])\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "y_proba = clf.predict_proba(data_test)\n",
    "weighted_costs = np.matmul(y_proba, np.array(C))\n",
    "y_pred = np.argmin(weighted_costs, axis = 1)\n",
    "\n",
    "#Archivo\n",
    "id = np.arange(data_test.shape[0])\n",
    "y = pd.DataFrame(\n",
    "     {\n",
    "          \"Id\": id,\n",
    "          \"Category\": y_pred,\n",
    "     }\n",
    ")\n",
    "nombre = \"prueba_XGBoost_prob.csv\"\n",
    "y.to_csv(nombre, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
